Fig.~\ref{fig:qual} illustrates the FOL explanations. Recall that the experts ($g$) in MoIE-CXR and the baselines are ELLs~\cite{barbiero2022entropy}, attributing attention weights to each concept. A concept with high attention weight indicates its high predictive significance. With a single $g$, the baselines rank the concepts in accordance with the identical order of attention weights for all the samples in a class, yielding  a generic FOL for that class. In Fig.~\ref{fig:qual}, the baseline PCBM + ELL uses \emph{left\_pleural} and \emph{pleural\_unspec} to identify effusion for all four samples. MoIE-CXR deploys multiple experts, learning to specialize in distinct subsets of a class. So different interpretable models in MoIE assign different attention weights to capture instance-specific concepts unique to each subset. In Fig.~\ref{fig:qual} expert2 relies on \emph{right\_pleural} and \emph{pleural\_unspec}, but expert4 relies only on \emph{pleural\_unspec} to classify effusion. 
The results show that the learned experts can provide more precise explanations at the subject level using the concepts, increasing confidence and trust in clinical use. 