Model generalizability is one of the main challenges of AI, exceptionally in high stake applications such as healthcare. While Neural Networks (NN) have achieved state-of-the-art (SOTA) performance in disease classification~\cite{irvin2019chexpert, rajpurkar2017chexnet, yu2022anatomy}, they are brittle to small shifts in the distribution~\cite{guan2021domain} caused by a change in acquisition protocol or scanner type. Fine-tuning a model on the target domain can alleviate this problem, but that requires a substantial amount of labeled data. On the contrary, radiologists follow fairly generalizable and comprehensible rules to read abnormality from an image. We develop a method to extract a mixture of interpretable models based on visual concepts, similar to radiologists' rules, from a pre-trained NN. Such a model is more data- and computation-efficient than the original model for fine-tuning to a new distribution.

Many factors cause the distribution shifts in CXR datasets, including changes in acquisition protocol and variations in scanner build~\cite{yan2020mri}. Fine-tuning the whole or part of a NN can help it adapt to a new domain~\cite{chu2016best}. However, this can require a significant amount of labeled data and be computationally expensive~\cite{wang2017growing, kandel2020deeply}. In contrast, radiologists search for patterns of changes in anatomy and apply logical rules based on their observations to rule out specific diagnoses. This approach is transparent and closer to an interpretable-by-design approach in AI.

Standard interpretable by design methods~\cite{rudin2022interpretable} aim to find an interpretable function (\eg linear regression or rule-based) between human-interpretable concepts and final output~\cite{koh2020concept}. A concept classifier~\cite{sarkar2021inducing, zarlenga2022concept} is used to detect the presence or absence of concepts in an image. In medical images, previous research uses TCAV scores~\cite{kim2017interpretability} to quantify the role of a concept on the final prediction~\cite{yeche2019ubs, graziani2020concept, clough2019global}, but the concept-based interpretable models have been mostly unexplored.
% and used for medical imaging purposes~\cite{yeche2019ubs, graziani2020concept, clough2019global}. 
Recently Posthoc Concept Bottleneck models (PCBMs)~\cite{yuksekgonul2022post} identify concepts from the embeddings of BB. However, the common design choice amongst those methods relies on a single interpretable classifier to explain the entire dataset, unable to capture the diverse sample-specific explanations and suboptimal performance. Further medical imaging interpretation is difficult due to disease subtypes, heterogeneous pathological patterns, etc.

\textbf{Our contributions.}
This paper proposes a novel data-efficient interpretable method that can be transferred between two domains. We begin with a BB and progressively extract a mixture of interpretable models and a residual from BB. Our method includes a set of selectors aiming to route the explainable samples through the interpretable models. The interpretable models provide First-order-logic (FOL) explanations for the samples they cover. The remaining unexplained samples are routed through the residuals until they are covered by a successive interpretable model. We repeat the process until we cover a desired fraction of data. Due to class imbalance in large CXR datasets, interpretable models from initial iterations cover samples with the disease (+ve samples), leaving subsequent iterations with samples without the disease (-ve samples). We address this problem by estimating the class-stratified coverage from the total data coverage. The target domain lacks concept-level annotation since concept annotation is expensive. Hence, we learn a concept detector in the target domain with a pseudo labeling approach~\cite{lee2013pseudo} and finetune the interpretable models. Our work is the first to apply concept-based methods to CXRs and transfer them between domains. 

% Our experiments demonstrate that our method (1) is able to capture diverse instance-specific concepts in the FOL explanations compared to all other interpretable models, (2) does not compromise the performance, (3) is able to identify ``harder'' samples, (4) can be efficiently transferred from source to target domain.
 
