Critical AI applications, \st healthcare necessitate the model explanations to foster trust and mitigate bias. To explain a model, we must choose from two mutually exclusive techniques -- explaining the decision of a flexible Blackbox (BB) post hoc or developing interpretable-by-design models. Post hoc explanations are susceptible to confirmation bias~\cite{wan2022explainability}, lack of  adherence to BB~\cite{adebayo2018sanity}, and insufficient mechanistic explanation of the decision-making process~\cite{rudin2019stop}. Interpretable-by-design models eliminate such issues but are not as flexible as BB. This paper bridges the gap between post hoc explanations and interpretable models.

Post hoc explanations include feature attribution~\cite{simonyan2013deep, selvaraju2017grad, smilkov2017smoothgrad} identifying critical features for the model prediction, counterfactual approaches~\cite{singla2019explanation, abid2021meaningfully} flipping the network's output and distillation methods~\cite{alharbi2021learning, cheng2020explaining} approximating with simpler functions around a small neighborhood. Post hoc methods retain the flexibility and performance of BB. Yet, due to a lack of mechanistic explanation~\cite{rudin2019stop}, post hoc explanations do not provide a recourse when an undesirable property of BB is identified. Interpretable models do not suffer from many such drawbacks. 

% For example, modern interpretable methods highlight human understandable \emph{concepts} that contribute to the downstream prediction.

% Interpretable models also have a long history in statistics and machine learning~\cite{letham2015interpretable, breiman1984classification}. 
% Early interpretable models are primarily designed for tabular data~\cite{hastie1987generalized, letham2015interpretable, breiman1984classification}. 
Modern interpretable models~\cite{koh2020concept, sarkar2021inducing, zarlenga2022concept} in computer vision first predict the humanly understandable \emph{concepts} from the input images, followed by labels from the concepts. Later, \cite{barbiero2022entropy} uses an entropy-based logical-linear model (ELL) to derive FOL explanations in terms of concepts. Recently Posthoc Concept Bottleneck models (PCBMs) ~\cite{yuksekgonul2022post} identify concepts from the embeddings of BB. Also, the hybrid PCBM (PCBM-h) learns a residual to mimic the performance of BB. All these methods rely on a single interpretable classifier to explain the entire dataset, restricting its flexibility, hindering performance, and incapability to capture the diverse sample-specific explanations. Recently the Route, Interpret and Repeat (RIR)~\cite{ghosh2023route} algorithm relaxes this assumption by iteratively extracting a mixture of interpretable models and a residual network from the given BB. They name interpretable models in each iteration as experts to offer FOL explanations, specializing in various subsets of data defined by it's  coverage. However, like all concept-based models, RIR is limited to computer vision datasets.

\textbf{Our contributions.}
 In this paper, we are the first to apply State-of-the-art (SOTA) concept-based interpretable models to a large CXR dataset. Also, we propose a novel stratified RIR algorithm to cover samples from a specific class to address the class imbalance in a large real-life CXR dataset. We estimate the coverages of positive and negative classes by multiplying the weight of a class with the given coverage. The weights of the class are the proportion of samples from training data. Our extensive experiments using MIMIC-CXR~\cite{12_johnsonmimic} demonstrate that our method is able to capture diverse instance-specific concepts in the FOLs compared to all other interpretable models. We next showcase the importance of the concepts quantitatively (1) by zeroing out them~\cite{ghosh2023route} and (2) during test time intervention~\cite{koh2020concept}. Finally, we hypothesize the concepts to be domain invariant. So we effectively transfer the models trained on MIMIC-CXR to Stanford-CXR~\cite{irvin2019chexpert} with minimal computation cost and limited Stanford-CXR training data.

% FOL is a logical function
% that accepts predicates (concept presence/absent) as input and returns a True/False output being a
% logical expression of the predicates. The logical expression, which is a set of AND, OR, Negative,
% and parenthesis, can be written in the so-called Disjunctive Normal Form (DNF). DNF is a FOL logical formula composed of a disjunction (OR) of conjunctions (AND), known as the ``sum of products''. 
% % Why the solution is smart / Did it work.

% 1. First to use concept bottleneck in CXR
% 2. First to apply CBM to domain transfer